{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set the plotting style to seaborn instead of matplotlib\n",
    "sns.set_theme()\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set the device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Free up cuda cache on reruns\n",
    "if device == 'cuda':\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.block = nn.Sequential([nn.Conv2d(in_channels, out_channels, kernel_size=3),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.MaxPool2d(2)])\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.block = nn.Sequential([ConvLayer(in_channels, 32),\n",
    "                                    ConvLayer(32, 64),\n",
    "                                    ConvLayer(64, 128),\n",
    "                                    ConvLayer(128, 256),\n",
    "                                    nn.Flatten()])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.block(x)\n",
    "    \n",
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module, sequence_length, *args, **kwargs):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.modules = nn.ModuleList([module(*args, **kwargs) for _ in range(sequence_length)])\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x is of shape (batch_size, sequence_length, channels, height, width)\n",
    "        return torch.stack([layer(x[:,i,:,:,:].squeeze()) for i, layer in enumerate(self.modules)], dim=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "class RecurrentBlock(nn.Module):\n",
    "    def __init__(self, sequence_length):\n",
    "        super(RecurrentBlock, self).__init__()\n",
    "        self.hidden = None\n",
    "        self.lstm = nn.LSTM(256, 128, bidirectional=True, batch_first=True, num_layers=sequence_length)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is of shape (batch_size, sequence_length, features)\n",
    "        output, self.hidden = self.lstm(x, self.hidden)\n",
    "        return output\n",
    "    \n",
    "\n",
    "class LinearBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearBlock, self).__init__()\n",
    "        self.block = nn.Sequential([nn.Linear(128, 64),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Dropout(0.5),\n",
    "                                   nn.Linear(64, 5),\n",
    "                                   nn.Sigmoid()])\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x is of shape (batch_size, features)\n",
    "        return self.block(x)\n",
    "    \n",
    "\n",
    "class FearNet(nn.Module):\n",
    "    def __init__(self, sequence_length=10):\n",
    "        super(FearNet, self).__init__()\n",
    "        self.tcnn = TimeDistributed(ConvBlock, sequence_length=sequence_length)\n",
    "        self.rnn = RecurrentBlock(sequence_length=sequence_length)\n",
    "        self.tail = LinearBlock()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x is of shape (batch_size, sequence_length, channels, height, width)\n",
    "        x = self.tcnn(x)\n",
    "        # x is of shape (batch_size, sequence_length, features)\n",
    "        self.rnn(x)\n",
    "        # Getting the last hidden state from the last layer\n",
    "        x = self.rnn.hidden[0][-1]\n",
    "        # x is of shape (batch_size, features)\n",
    "        x = self.tail(x)\n",
    "        return x\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
